}
run_sim <- function(nsim, N, n_lab, coeff, nn = 5000, eps_sd = 0.5) {
linear <- c()
nonlinear_1 <- c()
nonlinear_2 <- c()
for (nn in c(1:nsim)) {
set.seed(1234 + nn)
x <- rnorm(N, 0, 1)
eps <- rnorm(N, mean=0, sd=0.5)
y = 0.5*x + eps
linear <- rbind(linear, cal_bias_var(x, y, n_lab))
w <- 0.5*x + 0.5*x^2 + eps
nonlinear_1 <- rbind(nonlinear_1, cal_bias_var(x, w, n_lab))
z <- 0.5*x + 0.5*x^2 + 0.5*x^3 + eps
nonlinear_2 <- rbind(nonlinear_2, cal_bias_var(x, z, n_lab))
}
return(linear = linear, nonlinear_1 = nonlinear_1, nonlinear_2 = nonlinear_2)
}
m_0.5 <- run_sim(10000, 100, 0.5)
run_sim <- function(N, n_lab, coeff, nsim = 5000, eps_sd = 0.5) {
linear <- c()
nonlinear_1 <- c()
nonlinear_2 <- c()
for (nn in c(1:nsim)) {
set.seed(1234 + nn)
x <- rnorm(N, 0, 1)
eps <- rnorm(N, mean=0, sd=0.5)
y = coeff*x + eps
linear <- rbind(linear, cal_bias_var(x, y, n_lab))
w <- coeff*x + coeff*x^2 + eps
nonlinear_1 <- rbind(nonlinear_1, cal_bias_var(x, w, n_lab))
z <- coeff*x + coeff*x^2 + coeff*x^3 + eps
nonlinear_2 <- rbind(nonlinear_2, cal_bias_var(x, z, n_lab))
}
return(list(linear = linear, nonlinear_1 = nonlinear_1, nonlinear_2 = nonlinear_2))
}
m_0.5 <- run_sim(10000, 100, 0.5)
m_2 <- run_sim(10000, 100, 2)
m <- rbind(colMeans(m_0.5$linear), colMeans(m_2$linear))
m
m[, 9] <- m[, 8]
m[, 8] <- c(0.5, 0.5)
m[, 8]
m[, 9]
m <- rbind(colMeans(m_0.5$linear), colMeans(m_2$linear))
m_tmp <- m[, 8]
m[, 8] <- c(0.5, 0.5)
m <- cbind(m, m_tmp)
m
m <- rbind(colMeans(m_0.5$linear), colMeans(m_2$linear))
m_tmp <- m[, 8]
m[, 8] <- c(0.5, 0.5)
m <- cbind(m, m_tmp)
names(m) <- c("SL", "bias", "ESE", "MSE", "SSL", "Bias", "ESE", "ASE", "MSE")
rownames(m) <- c("m(x) = 0.5x", "m(x) = 2x")
m <- round(m, 4)
library(matrixStats)
library(kableExtra)
kableExtra::kbl(m, booktabs = T, caption="with random error 0.5") %>%
add_header_above(c(" ", "Supervised learning estimator"=4, "Semi-supervised estimator"=5)) %>%
kable_styling(latex_options = c("scale_down", "hold_position"))
m <- rbind(colMeans(m_0.5$linear), colMeans(m_2$linear))
m_tmp <- m[, 8]
m[, 8] <- c(0.5, 0.5)
m <- cbind(m, m_tmp)
colnames(m) <- c("SL", "bias", "ESE", "MSE", "SSL", "Bias", "ESE", "ASE", "MSE")
rownames(m) <- c("m(x) = 0.5x", "m(x) = 2x")
m <- round(m, 4)
library(matrixStats)
library(kableExtra)
kableExtra::kbl(m, booktabs = T, caption="with random error 0.5") %>%
add_header_above(c(" ", "Supervised learning estimator"=4, "Semi-supervised estimator"=5)) %>%
kable_styling(latex_options = c("scale_down", "hold_position"))
run_sim <- function(N, n_lab, coeff, nsim = 5000, eps_sd = 0.5) {
linear <- c()
nonlinear_1 <- c()
nonlinear_2 <- c()
for (nn in c(1:nsim)) {
set.seed(1234 + nn)
x <- rnorm(N, 0, 1)
eps <- rnorm(N, mean=0, sd=0.5)
y = coeff*x + eps
linear <- rbind(linear, cal_bias_var(x, y, n_lab))
w <- coeff*x + coeff*x^2 + eps
nonlinear_1 <- rbind(nonlinear_1, cal_bias_var(x, w, n_lab))
z <- coeff*x + coeff*x^2 + coeff*x^3 + eps
nonlinear_2 <- rbind(nonlinear_2, cal_bias_var(x, z, n_lab))
}
return(list(linear = linear, nonlinear_1 = nonlinear_1, nonlinear_2 = nonlinear_2))
}
m_0.5 <- run_sim(10000, 100, 0.5)
m_2 <- run_sim(10000, 100, 2)
m <- rbind(colMeans(m_0.5$linear), colMeans(m_2$linear))
m_tmp <- m[, 8]
m[, 8] <- c(0.5, 0.5)
m <- cbind(m, m_tmp)
colnames(m) <- c("SL", "bias", "ESE", "MSE", "SSL", "Bias", "ESE", "ASE", "MSE")
rownames(m) <- c("m(x) = 0.5x", "m(x) = 2x")
m <- round(m, 4)
kableExtra::kbl(m, booktabs = T, caption="with random error 0.5") %>%
add_header_above(c(" ", "Supervised learning estimator"=4, "Semi-supervised estimator"=5)) %>%
kable_styling(latex_options = c("scale_down", "hold_position"))
run_sim <- function(N, n_lab, coeff, nsim = 5000, eps_sd = 0.5) {
linear <- c()
nonlinear_1 <- c()
nonlinear_2 <- c()
for (nn in c(1:nsim)) {
set.seed(1234 + nn)
x <- rnorm(N, 0, 1)
eps <- rnorm(N, mean=0, sd=0.01)
y = coeff*x + eps
linear <- rbind(linear, cal_bias_var(x, y, n_lab))
w <- coeff*x + coeff*x^2 + eps
nonlinear_1 <- rbind(nonlinear_1, cal_bias_var(x, w, n_lab))
z <- coeff*x + coeff*x^2 + coeff*x^3 + eps
nonlinear_2 <- rbind(nonlinear_2, cal_bias_var(x, z, n_lab))
}
return(list(linear = linear, nonlinear_1 = nonlinear_1, nonlinear_2 = nonlinear_2))
}
m_0.5 <- run_sim(10000, 100, 0.5)
m_2 <- run_sim(10000, 100, 2)
m <- rbind(colMeans(m_0.5$linear), colMeans(m_2$linear))
m_tmp <- m[, 8]
m[, 8] <- c(0.5, 0.5)
m <- cbind(m, m_tmp)
colnames(m) <- c("SL", "bias", "ESE", "MSE", "SSL", "Bias", "ESE", "ASE", "MSE")
rownames(m) <- c("m(x) = 0.5x", "m(x) = 2x")
m <- round(m, 4)
kableExtra::kbl(m, booktabs = T, caption="with random error 0.5") %>%
add_header_above(c(" ", "Supervised learning estimator"=4, "Semi-supervised estimator"=5)) %>%
kable_styling(latex_options = c("scale_down", "hold_position"))
run_sim <- function(N, n_lab, coeff, nsim = 5000, eps_sd = 0.5) {
linear <- c()
nonlinear_1 <- c()
nonlinear_2 <- c()
for (nn in c(1:nsim)) {
set.seed(1234 + nn)
x <- rnorm(N, 0, 1)
eps <- rnorm(N, mean=0, sd=0.5)
y = coeff*x + eps
linear <- rbind(linear, cal_bias_var(x, y, n_lab))
w <- coeff*x + coeff*x^2 + eps
nonlinear_1 <- rbind(nonlinear_1, cal_bias_var(x, w, n_lab))
z <- coeff*x + coeff*x^2 + coeff*x^3 + eps
nonlinear_2 <- rbind(nonlinear_2, cal_bias_var(x, z, n_lab))
}
return(list(linear = linear, nonlinear_1 = nonlinear_1, nonlinear_2 = nonlinear_2))
}
m_0.5 <- run_sim(10000, 100, 0.5)
m_2 <- run_sim(10000, 100, 2)
m <- rbind(colMeans(m_0.5$linear), colMeans(m_2$linear))
m_tmp <- m[, 8]
m[, 8] <- c(0.5, 0.5)
m <- cbind(m, m_tmp)
colnames(m) <- c("SL", "bias", "ESE", "MSE", "SSL", "Bias", "ESE", "ASE", "MSE")
rownames(m) <- c("m(x) = 0.5x", "m(x) = 2x")
m <- round(m, 4)
kableExtra::kbl(m, booktabs = T, caption="with random error 0.5") %>%
add_header_above(c(" ", "Supervised learning estimator"=4, "Semi-supervised estimator"=5)) %>%
kable_styling(latex_options = c("scale_down", "hold_position"))
m <- rbind(colMeans(m_0.5$linear), colMeans(m_2$linear))
m_tmp <- m[, 8]
m[, 8] <- c(0.5, 2)
m <- cbind(m, m_tmp)
colnames(m) <- c("SL", "bias", "ESE", "MSE", "SSL", "Bias", "ESE", "ASE", "MSE")
rownames(m) <- c("m(x) = 0.5x", "m(x) = 2x")
m <- round(m, 4)
kableExtra::kbl(m, booktabs = T, caption="with random error 0.5") %>%
add_header_above(c(" ", "Supervised learning estimator"=4, "Semi-supervised estimator"=5)) %>%
kable_styling(latex_options = c("scale_down", "hold_position"))
m <- rbind(colMeans(m_0.5$linear), colMeans(m_2$linear))
m_tmp <- m[, 8]
m[, 8] <- c(0.5, 0.5)
m <- cbind(m, m_tmp)
colnames(m) <- c("SL", "bias", "ESE", "MSE", "SSL", "Bias", "ESE", "ASE", "MSE")
rownames(m) <- c("m(x) = 0.5x", "m(x) = 2x")
m <- round(m, 4)
kableExtra::kbl(m, booktabs = T, caption="with random error 0.5") %>%
add_header_above(c(" ", "Supervised learning estimator"=4, "Semi-supervised estimator"=5)) %>%
kable_styling(latex_options = c("scale_down", "hold_position"))
t <- rbind(colMeans(m_0.5$linear), colMeans(m_0.5$nonlinear_1), colMeans(m_0.5$nonlinear_2))
t
t <- rbind(colMeans(m_0.5$linear), colMeans(m_0.5$nonlinear_1), colMeans(m_0.5$nonlinear_2))
t <- t[, c(4, 9)]
t[, ]
t[, 1]
t <- rbind(colMeans(m_0.5$linear), colMeans(m_0.5$nonlinear_1), colMeans(m_0.5$nonlinear_2))
t <- t[, c(4, 8)]
t <- rbind(colMeans(m_0.5$linear), colMeans(m_0.5$nonlinear_1), colMeans(m_0.5$nonlinear_2))
t <- cbind(t[, c(4, 8)], t[, 4]/t[, 8])
t
t <- rbind(colMeans(m_0.5$linear), colMeans(m_0.5$nonlinear_1), colMeans(m_0.5$nonlinear_2))
t <- cbind(t[, c(4, 8)], t[, 4]/t[, 8])
colnames(t) <- c("MSE", "MSE", "Efficiency")
t <- rbind(colMeans(m_0.5$linear), colMeans(m_0.5$nonlinear_1), colMeans(m_0.5$nonlinear_2))
t <- cbind(t[, c(4, 8)], t[, 4]/t[, 8])
colnames(t) <- c("MSE", "MSE", "Efficiency")
kableExtra::kbl(t, booktabs = T, caption="with random error 0.5") %>%
add_header_above(c(" ", "Supervised learning estimator"=1, "Semi-supervised estimator"=1)) %>%
kable_styling(latex_options = c("scale_down", "hold_position"))
t <- rbind(colMeans(m_0.5$linear), colMeans(m_0.5$nonlinear_1), colMeans(m_0.5$nonlinear_2))
t <- cbind(t[, c(4, 8)], t[, 4]/t[, 8])
colnames(t) <- c("MSE", "MSE", "Efficiency")
rownames(t) <- c("linear", "non-linear-1", "non-linear-2")
kableExtra::kbl(t, booktabs = T, caption="with random error 0.5") %>%
add_header_above(c(" ", "Supervised learning estimator"=1, "Semi-supervised estimator"=1)) %>%
kable_styling(latex_options = c("scale_down", "hold_position"))
t
t <- rbind(colMeans(m_0.5$linear), colMeans(m_0.5$nonlinear_1), colMeans(m_0.5$nonlinear_2))
t <- cbind(t[, c(4, 8)], t[, 4]/t[, 8])
colnames(t) <- c("MSE", "MSE", "Efficiency")
rownames(t) <- c("linear", "non-linear I", "non-linear II")
t <- rbind(colMeans(m_0.5$linear), colMeans(m_0.5$nonlinear_1), colMeans(m_0.5$nonlinear_2))
t <- cbind(t[, c(4, 8)], t[, 4]/t[, 8])
colnames(t) <- c("MSE", "MSE", "Efficiency")
rownames(t) <- c("linear", "non-linear I", "non-linear II")
kableExtra::kbl(t, booktabs = T, caption="with random error 0.5") %>%
add_header_above(c(" ", "Supervised learning estimator"=1, "Semi-supervised estimator"=1)) %>%
kable_styling(latex_options = c("scale_down", "hold_position"))
t <- rbind(colMeans(m_0.5$linear), colMeans(m_0.5$nonlinear_1), colMeans(m_0.5$nonlinear_2))
t <- cbind(t[, c(4, 8)], t[, 4]/t[, 8])
colnames(t) <- c("MSE", "MSE", "Efficiency")
rownames(t) <- c("linear", "non-linear I", "non-linear II")
kableExtra::kbl(t, booktabs = T, caption="with random error 0.5") %>%
add_header_above(c(" ", " ", "Supervised learning estimator"=1, "Semi-supervised estimator"=1)) %>%
kable_styling(latex_options = c("scale_down", "hold_position"))
t <- rbind(colMeans(m_0.5$linear), colMeans(m_0.5$nonlinear_1), colMeans(m_0.5$nonlinear_2))
t <- cbind(t[, c(4, 8)], t[, 4]/t[, 8])
colnames(t) <- c("MSE", "MSE", "Efficiency")
rownames(t) <- c("linear", "non-linear I", "non-linear II")
kableExtra::kbl(t, booktabs = T, caption="with random error 0.5") %>%
add_header_above(c(" ", "Supervised learning estimator"=1, "Semi-supervised estimator"=1, " ")) %>%
kable_styling(latex_options = c("scale_down", "hold_position"))
t <- rbind(colMeans(m_0.5$linear), colMeans(m_0.5$nonlinear_1), colMeans(m_0.5$nonlinear_2))
t <- cbind(t[, c(4, 8)], t[, 4]/t[, 8])
colnames(t) <- c("MSE", "MSE", "Efficiency")
rownames(t) <- c("linear", "non-linear I", "non-linear II")
kableExtra::kbl(t, booktabs = T, caption="Efficiencies with respect to empirical mean square error") %>%
add_header_above(c(" ", "Supervised learning estimator"=1, "Semi-supervised estimator"=1, " ")) %>%
kable_styling(latex_options = c("scale_down", "hold_position"))
kableExtra::kbl(m, booktabs = T, caption="with random error 0.5") %>%
add_header_above(c(" ", "Supervised learning estimator"=4, "Semi-supervised estimator"=5)) %>%
kable_styling(latex_options = c("scale_down", "hold_position"))
m <- rbind(colMeans(m_0.5$linear), colMeans(m_2$linear))
m_tmp <- m[, 8]
m[, 8] <- c(0.5, 0.5)
m <- cbind(m, m_tmp)
colnames(m) <- c("SL", "bias", "ESE", "MSE", "SSL", "Bias", "ESE", "ASE", "MSE")
rownames(m) <- c("m(x) = 0.5x", "m(x) = 2x")
m <- round(m, 3)
kableExtra::kbl(m, booktabs = T, caption="with random error 0.5") %>%
add_header_above(c(" ", "Supervised learning estimator"=4, "Semi-supervised estimator"=5)) %>%
kable_styling(latex_options = c("scale_down", "hold_position"))
t <- rbind(colMeans(m_0.5$linear), colMeans(m_0.5$nonlinear_1), colMeans(m_0.5$nonlinear_2))
t <- cbind(t[, c(4, 8)], t[, 4]/t[, 8])
colnames(t) <- c("MSE", "MSE", "Efficiency")
rownames(t) <- c("linear", "non-linear I", "non-linear II")
kableExtra::kbl(t, booktabs = T, caption="Efficiencies with respect to empirical mean square error") %>%
add_header_above(c(" ", "Supervised learning estimator"=1, "Semi-supervised estimator"=1, " ")) %>%
kable_styling(latex_options = c("scale_down", "hold_position"))
t <- rbind(colMeans(m_0.5$linear), colMeans(m_0.5$nonlinear_1), colMeans(m_0.5$nonlinear_2))
t <- cbind(t[, c(4, 8)], t[, 4]/t[, 8])
colnames(t) <- c("MSE", "MSE", "Efficiency")
rownames(t) <- c("linear", "non-linear I", "non-linear II")
t <- round(t, 3)
kableExtra::kbl(t, booktabs = T, caption="Efficiencies with respect to empirical mean square error") %>%
add_header_above(c(" ", "Supervised learning estimator"=1, "Semi-supervised estimator"=1, " ")) %>%
kable_styling(latex_options = c("scale_down", "hold_position"))
kableExtra::kbl(m, booktabs = T, caption="with random error 0.5") %>%
add_header_above(c(" ", "Supervised learning estimator"=4, "Semi-supervised estimator"=5)) %>%
kable_styling(latex_options = c("scale_down", "hold_position"))
t <- rbind(colMeans(m_2$linear), colMeans(m_2$nonlinear_1), colMeans(m_2$nonlinear_2))
t <- cbind(t[, c(4, 8)], t[, 4]/t[, 8])
colnames(t) <- c("MSE", "MSE", "Efficiency")
rownames(t) <- c("linear", "non-linear I", "non-linear II")
t <- round(t, 3)
kableExtra::kbl(t, booktabs = T, caption="Efficiencies with respect to empirical mean square error for m(x) = 0.5x") %>%
add_header_above(c(" ", "Supervised learning estimator"=1, "Semi-supervised estimator"=1, " ")) %>%
kable_styling(latex_options = c("scale_down", "hold_position"))
t <- rbind(colMeans(m_2$linear), colMeans(m_2$nonlinear_1), colMeans(m_2$nonlinear_2))
t <- cbind(t[, c(4, 8)], t[, 4]/t[, 8])
colnames(t) <- c("MSE", "MSE", "Efficiency")
rownames(t) <- c("linear", "non-linear I", "non-linear II")
t <- round(t, 3)
kableExtra::kbl(t, booktabs = T, caption="Efficiencies with respect to empirical mean square error for m(x) = 2x") %>%
add_header_above(c(" ", "Supervised learning estimator"=1, "Semi-supervised estimator"=1, " ")) %>%
kable_styling(latex_options = c("scale_down", "hold_position"))
kableExtra::kbl(m, booktabs = T, caption="with random error 0.5") %>%
add_header_above(c(" ", "Supervised learning estimator"=4, "Semi-supervised estimator"=5)) %>%
kable_styling(latex_options = c("scale_down", "hold_position"))
lab_100_beta_0.5 <- run_sim(10000, 100, 0.5)
lab_200_beta_0.5 <- run_sim(10000, 200, 0.5)
lab_400_beta_0.5 <- run_sim(10000, 400, 0.5)
table1 <- rbind(colMeans(lab_100_beta_0.5$linear), colMeans(lab_200_beta_0.5$linear), colMeans(lab_200_beta_0.5$linear))
table1 <- rbind(colMeans(lab_100_beta_0.5$linear), colMeans(lab_200_beta_0.5$linear), colMeans(lab_200_beta_0.5$linear))
table1
table1 <- rbind(colMeans(lab_100_beta_0.5$linear), colMeans(lab_200_beta_0.5$linear), colMeans(lab_400_beta_0.5$linear))
rownames(table1) <- c(100, 200, 400)
table1 <- rbind(colMeans(lab_100_beta_0.5$linear), colMeans(lab_200_beta_0.5$linear), colMeans(lab_400_beta_0.5$linear))
rownames(table1) <- c(100, 200, 400)
table1
library(matrixStats)
library(kableExtra)
cal_bias_var <- function(x, y, label.size) {
x.lab <- x[1: label.size]
y.lab <- y[1: label.size]
mod <- lm(y.lab~x.lab)
sl <- mean(y.lab)
sl.bias <- sl - mean(y)
sl.sd <- sd(y.lab)
x.unlab <- x[(label.size+1):length(y)]
# m estimator
mx <- summary(mod)$coef[1,1] + summary(mod)$coef[2,1] * x.unlab
ssl <- mean(mx)
ssl.bias <- ssl - mean(y)
ssl.sd <- sd(mx)
sl.mse <- sl.sd^2 + sl.bias^2
ssl.mse <- ssl.sd^2 + sl.bias^2
return(c(sl, sl.bias, sl.sd, sl.mse, ssl, ssl.bias, ssl.sd, ssl.ase = 0.5, ssl.mse))
}
run_sim <- function(N, n_lab, coeff, nsim = 5000, eps_sd = 0.5) {
linear <- c()
nonlinear_1 <- c()
nonlinear_2 <- c()
for (nn in c(1:nsim)) {
set.seed(1234 + nn)
x <- rnorm(N, 0, 1)
eps <- rnorm(N, mean=0, sd=0.5)
y = coeff*x + eps
linear <- rbind(linear, cal_bias_var(x, y, n_lab))
w <- coeff*x + coeff*x^2 + eps
nonlinear_1 <- rbind(nonlinear_1, cal_bias_var(x, w, n_lab))
z <- coeff*x + coeff*x^2 + coeff*x^3 + eps
nonlinear_2 <- rbind(nonlinear_2, cal_bias_var(x, z, n_lab))
}
return(list(linear = linear, nonlinear_1 = nonlinear_1, nonlinear_2 = nonlinear_2))
}
# Setting 1: compare n = 100, 200, 400
lab_100_beta_0.5 <- run_sim(10000, 100, 0.5)
lab_200_beta_0.5 <- run_sim(10000, 200, 0.5)
lab_400_beta_0.5 <- run_sim(10000, 400, 0.5)
table1 <- rbind(colMeans(lab_100_beta_0.5$linear), colMeans(lab_200_beta_0.5$linear), colMeans(lab_400_beta_0.5$linear))
rownames(table1) <- c(100, 200, 400)
colnames(table1) <- c("SL", "bias", "ESE", "MSE", "SSL", "Bias", "ESE", "ASE", "MSE")
table1
table1 <- rbind(colMeans(lab_100_beta_0.5$linear), colMeans(lab_200_beta_0.5$linear), colMeans(lab_400_beta_0.5$linear))
rownames(table1) <- c(100, 200, 400)
colnames(table1) <- c("SL", "bias", "ESE", "MSE", "SSL", "Bias", "ESE", "ASE", "MSE")
round(table1, 3)
table1 <- rbind(colMeans(lab_100_beta_0.5$linear), colMeans(lab_200_beta_0.5$linear), colMeans(lab_400_beta_0.5$linear))
rownames(table1) <- c(100, 200, 400)
colnames(table1) <- c("SL", "bias", "ESE", "MSE", "SSL", "Bias", "ESE", "ASE", "MSE")
round(table1, 4)
table1[, 4] / table1[, 9]
table1 <- rbind(colMeans(lab_100_beta_0.5$linear), colMeans(lab_200_beta_0.5$linear), colMeans(lab_400_beta_0.5$linear))
rownames(table1) <- c(100, 200, 400)
colnames(table1) <- c("SL", "bias", "ESE", "MSE", "SSL", "Bias", "ESE", "ASE", "MSE")
round(table1, 4)
table1[, 4] / table1[, 9]
table1 <- rbind(colMeans(lab_100_beta_0.5$linear), colMeans(lab_200_beta_0.5$linear), colMeans(lab_400_beta_0.5$linear))
rownames(table1) <- c(100, 200, 400)
colnames(table1) <- c("SL", "bias", "ESE", "MSE", "SSL", "Bias", "ESE", "ASE", "MSE")
Efficiency <- table1[, 4] / table1[, 9]
table1 <- cbind(table1, Efficiency)
table1 <- round(table1, 4)
table1 <- rbind(colMeans(lab_100_beta_0.5$linear), colMeans(lab_200_beta_0.5$linear), colMeans(lab_400_beta_0.5$linear))
rownames(table1) <- c(100, 200, 400)
colnames(table1) <- c("SL", "bias", "ESE", "MSE", "SSL", "Bias", "ESE", "ASE", "MSE")
Efficiency <- table1[, 4] / table1[, 9]
table1 <- cbind(table1, Efficiency)
table1 <- round(table1, 4)
table1
table1 <- rbind(colMeans(lab_100_beta_0.5$linear), colMeans(lab_200_beta_0.5$linear), colMeans(lab_400_beta_0.5$linear))
rownames(table1) <- c(100, 200, 400)
colnames(table1) <- c("SL", "bias", "ESE", "MSE", "SSL", "Bias", "ESE", "ASE", "MSE")
Efficiency <- table1[, 4] / table1[, 9]
table1 <- cbind(table1, Efficiency)
table1 <- round(table1, 4)
table1
kableExtra::kbl(table1, booktabs = T, caption="Efficiencies with respect to empirical mean square error for m(x) = 0.5x") %>%
add_header_above(c(" ", "Supervised learning estimator"=4, "Semi-supervised estimator"=5, " ")) %>%
kable_styling(latex_options = c("scale_down", "hold_position"))
lab_100_beta_2 <- run_sim(10000, 100, 2)
# Setting 3: compare m(x) = 0.5 and m(x) = 2
lab_400_beta_2 <- run_sim(10000, 400, 2)
table3 <- rbind(colMeans(lab_400_beta_2$linear), colMeans(lab_400_beta_2$linear))
rownames(m) <- c("m(x) = 0.5x", "m(x) = 2x")
table3 <- rbind(colMeans(lab_400_beta_2$linear), colMeans(lab_400_beta_2$linear))
rownames(table3) <- c("m(x) = 0.5x", "m(x) = 2x")
colnames(table3) <- c("SL", "bias", "ESE", "MSE", "SSL", "Bias", "ESE", "ASE", "MSE")
Efficiency <- table3[, 4] / table1[, 9]
table3 <- cbind(table3, Efficiency)
table3 <- round(table3, 4)
table3 <- rbind(colMeans(lab_400_beta_2$linear), colMeans(lab_400_beta_2$linear))
rownames(table3) <- c("m(x) = 0.5x", "m(x) = 2x")
colnames(table3) <- c("SL", "bias", "ESE", "MSE", "SSL", "Bias", "ESE", "ASE", "MSE")
Efficiency <- table3[, 4] / table3[, 9]
table3 <- cbind(table3, Efficiency)
table3 <- round(table3, 4)
table3 <- rbind(colMeans(lab_400_beta_2$linear), colMeans(lab_400_beta_2$linear))
rownames(table3) <- c("m(x) = 0.5x", "m(x) = 2x")
colnames(table3) <- c("SL", "bias", "ESE", "MSE", "SSL", "Bias", "ESE", "ASE", "MSE")
Efficiency <- table3[, 4] / table3[, 9]
table3 <- cbind(table3, Efficiency)
table3 <- round(table3, 4)
kableExtra::kbl(table3, booktabs = T, caption="Efficiencies with respect to empirical mean square error for m(x) = 0.5x") %>%
add_header_above(c(" ", "Supervised learning estimator"=4, "Semi-supervised estimator"=5, " ")) %>%
kable_styling(latex_options = c("scale_down", "hold_position"))
table3 <- rbind(colMeans(lab_400_beta_0.5$linear), colMeans(lab_400_beta_2$linear))
rownames(table3) <- c("m(x) = 0.5x", "m(x) = 2x")
colnames(table3) <- c("SL", "bias", "ESE", "MSE", "SSL", "Bias", "ESE", "ASE", "MSE")
Efficiency <- table3[, 4] / table3[, 9]
table3 <- cbind(table3, Efficiency)
table3 <- round(table3, 4)
kableExtra::kbl(table3, booktabs = T, caption="Efficiencies with respect to empirical mean square error for m(x) = 0.5x") %>%
add_header_above(c(" ", "Supervised learning estimator"=4, "Semi-supervised estimator"=5, " ")) %>%
kable_styling(latex_options = c("scale_down", "hold_position"))
# Setting 2: N=10000 down to n = 100
r <- c()
for (i in seq(100, 10000, 100, nsim = 1000)) {
r <- rbind(r, colMeans(run_sim(i, 100, 0.5)$linear))
}
# Setting 2: N=10000 down to n = 100
r <- c()
for (i in seq(100, 10000, 100)) {
r <- rbind(r, colMeans(run_sim(i, 100, 0.5, nsim = 1000)$linear))
}
Efficiency <- r[, 4] / r[, 9]
r
Efficiency
r
Efficiency <- r[, 4] / r[, 9]
Efficiency[1] <- 1
plot(c(1:100), Efficiency)
Efficiency <- r[, 4] / r[, 9]
Efficiency[1] <- 1
data.frame(index = c(1:100), eff = Efficiency) %>%
ggplot() + geom_point(aes(index, eff))
Efficiency <- r[, 4] / r[, 9]
Efficiency[1] <- 1
data.frame(index = c(1:100), eff = Efficiency) %>%
ggplot2() + geom_point(aes(index, eff))
library(ggplot2)
Efficiency <- r[, 4] / r[, 9]
Efficiency[1] <- 1
data.frame(index = c(1:100), eff = Efficiency) %>%
ggplot() + geom_point(aes(index, eff))
library(ggplot2)
Efficiency <- r[, 4] / r[, 9]
Efficiency[1] <- 1
data.frame(index = c(1:100), eff = Efficiency) %>%
ggplot() + geom_point(aes(index, eff)) + ylim(1,75, 2)
library(ggplot2)
Efficiency <- r[, 4] / r[, 9]
Efficiency[1] <- 1
data.frame(index = c(1:100), eff = Efficiency) %>%
ggplot() + geom_point(aes(index, eff)) + ylim(c(1.75, 2))
library(ggplot2)
Efficiency <- r[, 4] / r[, 9]
Efficiency[1] <- 1
data.frame(index = c(1:100), eff = Efficiency) %>%
ggplot() + geom_point(aes(index, eff)) + ylim(c(1.75, 2)) +
ylab("Relative efficiency (SL:SSL)") + xlab("Unlabeled sample size N")
library(ggplot2)
Efficiency <- r[, 4] / r[, 9]
Efficiency[1] <- 1
data.frame(index = c(1:100), eff = Efficiency) %>%
ggplot() + geom_point(aes(index, eff)) + ylim(c(1.75, 2)) +
ylab("Relative efficiency (SL:SSL)") + xlab("Unlabeled sample size N") + ggtitle("N = 100 to N = 100000")
setwd("~/Desktop")
knitr::opts_chunk$set(echo = TRUE)
journals <- read.csv(file = 'csv-machinelea-set.csv')
journals <- read.csv(file = 'csv-machinelea-set.csv')
head(journals)
library(tidyverse)
journals %>%
group_by(Publication.Year)
library(tidyverse)
journals %>%
group_by(Publication.Year) %>%
summarise(count = n())
journals %>%
group_by(Journal.Book) %>%
summarise(count = n())
journals %>%
group_by(Journal.Book) %>%
summarise(count = n()) %>%
sort(count)
journals %>%
group_by(Journal.Book) %>%
summarise(count = n()) %>%
arrange(desc(count))
abstract <- read.table("abstract-antioxidan-set.txt")
abstract <- scan("abstract-antioxidan-set.txt")
abstract <- scan("abstract-antioxidan-set.txt")
abstract <- read.table("abstract-antioxidan-set.txt")
abstract <- read.table("abstract-machinelea-set.txt")
setwd("~/Documents/PhD supervise/project/Untitled Folder")
journals <- read.csv(file = 'new_machine_result.csv')
head(journals)
library(tidyverse)
journals %>%
group_by(Publication.Year) %>%
summarise(count = n())
journals %>%
group_by(Journal.Book) %>%
summarise(count = n()) %>%
arrange(desc(count))
journals %>%
group_by(Journal.Book) %>%
summarise(count = n()) %>%
arrange(desc(count))
